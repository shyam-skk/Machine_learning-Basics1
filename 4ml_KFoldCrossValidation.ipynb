{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"4ml_KFoldCrossValidation.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"QBA9lYxJdQxD","colab_type":"text"},"source":["# K-Fold Cross Validation\n","**Cross validation is using for prevent overfitting. Here we are using K-Fold Cross Validation with Support Vector Machines.**\n","\n","Cross validation (CV) is one of the technique used to test the effectiveness of a machine learning models, it is also a re-sampling procedure used to evaluate a model if we have a limited data. K-Fold CV is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point. ."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"xwONidTLdQxP","colab_type":"code","colab":{}},"source":["import numpy as np                                    # import numpy for  arrays and matrices operation.\n","from sklearn import model_selection                   # importing sklearn model selection for Split arrays or matrices into random train and test subsets\n","from sklearn.model_selection import cross_val_score   # importing cross_val_score for cross validation score measurement and metrices.\n","from sklearn.model_selection import train_test_split  # for split arrays or matrices into random train and test subsets\n","from sklearn import datasets                          # imporrt sklearn datasets for loading prebuild datasets.\n","from sklearn import svm                               # import sklearn svm module for analyze data used for classification"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HObIQ3nVNaB","colab_type":"text"},"source":["To disable the python future warings on packages and libraries we will use warning packages. It will keep the code clean. But its nor recommended."]},{"cell_type":"code","metadata":{"id":"NQU3k8fvn2YJ","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIDpht-we9Dv","colab_type":"text"},"source":["**Here I am using prebuilt -  wine - dataset in sklearn.**"]},{"cell_type":"code","metadata":{"id":"VUrnd89olo9D","colab_type":"code","outputId":"93aa041a-98ea-4e8a-f1be-03273a7f9dc6","executionInfo":{"status":"ok","timestamp":1573138533553,"user_tz":0,"elapsed":843,"user":{"displayName":"Shyam Krishnan k","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDvbcu57SSn8rGyaigikU0Ikt8gSPunDjdH3GRiuA=s64","userId":"01326725485708424731"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["dataset = datasets.load_wine()  # loading the wine dataset as numpy array.\n","print(dataset)                  # pring the array values."],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n","        1.065e+03],\n","       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n","        1.050e+03],\n","       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n","        1.185e+03],\n","       ...,\n","       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n","        8.350e+02],\n","       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n","        8.400e+02],\n","       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n","        5.600e+02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2]), 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7'), 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 178 (50 in each of three classes)\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- Alcohol\\n \\t\\t- Malic acid\\n \\t\\t- Ash\\n\\t\\t- Alcalinity of ash  \\n \\t\\t- Magnesium\\n\\t\\t- Total phenols\\n \\t\\t- Flavanoids\\n \\t\\t- Nonflavanoid phenols\\n \\t\\t- Proanthocyanins\\n\\t\\t- Color intensity\\n \\t\\t- Hue\\n \\t\\t- OD280/OD315 of diluted wines\\n \\t\\t- Proline\\n\\n    - class:\\n            - class_0\\n            - class_1\\n            - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\n.. topic:: References\\n\\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \\n  Comparison of Classifiers in High Dimensional Settings, \\n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Technometrics). \\n\\n  The data was used with many others for comparing various \\n  classifiers. The classes are separable, though only RDA \\n  has achieved 100% correct classification. \\n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n  (All results using the leave-one-out technique) \\n\\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \\n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \\n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Journal of Chemometrics).\\n', 'feature_names': ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qAAPdBWFXGho","colab_type":"code","outputId":"2486cc52-eb2a-4750-ab0e-9d2a06929963","executionInfo":{"status":"ok","timestamp":1573138536976,"user_tz":0,"elapsed":844,"user":{"displayName":"Shyam Krishnan k","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDvbcu57SSn8rGyaigikU0Ikt8gSPunDjdH3GRiuA=s64","userId":"01326725485708424731"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["print(dataset.data.shape)     # return the shape of the dataset.\n","print('\\n')\n","print(dataset.feature_names)  # return the name of the columns in the dataset.\n","print('\\n')\n","print(dataset.target_names)   # return the values in the target column.\n","print('\\n')\n","print(dataset.target)         # print the target set values."],"execution_count":0,"outputs":[{"output_type":"stream","text":["(178, 13)\n","\n","\n","['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n","\n","\n","['class_0' 'class_1' 'class_2']\n","\n","\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jL4P3lxqXgQu","colab_type":"text"},"source":["Dataset comprises of 13 features (alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline) and type of wine cultivar. This data has three type of wine Class_0, Class_1, and Class_3. Here you can build a model to classify the type of wine. "]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"h4k3_DuYdQxe","colab_type":"code","outputId":"8ca6cace-16ba-4927-f561-5f9c82d9873e","executionInfo":{"status":"ok","timestamp":1573139394856,"user_tz":0,"elapsed":821,"user":{"displayName":"Shyam Krishnan k","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDvbcu57SSn8rGyaigikU0Ikt8gSPunDjdH3GRiuA=s64","userId":"01326725485708424731"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Split the wine data into train/test data sets with 40% reserved for testing\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(dataset.data, dataset.target, test_size=0.4, random_state=0)\n","\n","# Build an SVC model for predicting wine classifications using training data\n","clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n","\n","# Now measure its performance with the test data\n","clf.score(X_test, y_test)   "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9583333333333334"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"jsNFlVn6Y3um","colab_type":"text"},"source":["**For our model SVM classification model without cross validation, the classification accuracy is : 0.958 ie 95.83%.**"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"dri-enHhdQxo","colab_type":"text"},"source":["A single train/test split is made easy with the train_test_split function in the cross_validation library. We will split the dataset in to 60:40 propotion by keeping the 40% for test data. We  will assign more data for training to train our model well enough other wise having less amount of data for traning will leads to underfitting."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"KkzBFqxYdQxq","colab_type":"code","outputId":"dd90f841-4cbc-49e4-88bc-7578a3415090","executionInfo":{"status":"ok","timestamp":1573146381392,"user_tz":0,"elapsed":1249,"user":{"displayName":"Shyam Krishnan k","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDvbcu57SSn8rGyaigikU0Ikt8gSPunDjdH3GRiuA=s64","userId":"01326725485708424731"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# We give cross_val_score a model, the entire data set and its \"real\" values, and the number of folds:\n","scores = cross_val_score(clf, dataset.data, dataset.target, cv=5)\n","\n","# Print the accuracy for each fold:\n","print(scores)\n","\n","# And the mean accuracy of all 5 folds:\n","print(scores.mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0.94594595 0.88888889 0.97222222 1.         1.        ]\n","0.9614114114114114\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f-yylZcdZwbc","colab_type":"text"},"source":["K-Fold cross validation is just as easy; let's use a K of 5.\n","\n","Here we are using scenario of 5-Fold cross validation(K=5). Here, the data set is split into 5 folds. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set\n","\n","For our first model with fold value 5, the, the performance measure of each fold is given. **The overall average classification accuracy is 96.18%.** From the comparision its clear that the performance of the model is improved (for older model the score was 95.83%)."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"I0YDwjbbdQyX","colab_type":"code","outputId":"79fbee7c-3d87-4507-db7b-5e51124f21ae","executionInfo":{"status":"ok","timestamp":1573143784844,"user_tz":0,"elapsed":1873,"user":{"displayName":"Shyam Krishnan k","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDvbcu57SSn8rGyaigikU0Ikt8gSPunDjdH3GRiuA=s64","userId":"01326725485708424731"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Build an SVC model for predicting wine classifications using training data\n","clf = svm.SVC(kernel='poly', C=1).fit(X_train, y_train)\n","\n","# Now measure its performance with the test data\n","clf.score(X_test, y_test)   "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9722222222222222"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"iDFZx4MCdJDo","colab_type":"text"},"source":["Here when we change the kernael of the SVM to polynomial from linear with the same number of fold, **the classification performance is increased to 0.972**, which is better than the second model witth 5 folds and linear kernal.\n","\n","*The linear and polynomial are simply different in case of making the hyperplane decision boundary between the classes.\n","The kernel functions are used to map the original dataset (linear/nonlinear ) into a higher dimensional space with view to making it linear dataset.\n","Usually linear and polynomial kernels are less time consuming.*"]}]}